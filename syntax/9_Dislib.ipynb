{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dislib tutorial\n",
    "\n",
    "This tutorial will show the basics of using [dislib](https://dislib.bsc.es).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Apart from dislib, this notebook requires [PyCOMPSs 2.5](https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar/).\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "\n",
    "First, we need to start an interactive PyCOMPSs session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycompss.interactive as ipycompss\n",
    "ipycompss.start(graph=True, monitor=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import dislib and we are all set to start working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dislib as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed arrays\n",
    "\n",
    "The main data structure in dislib is the distributed array (or ds-array). These arrays are a distributed representation of a 2-dimensional array that can be operated as a regular Python object. Usually, rows in the array represent samples, while columns represent features.\n",
    "\n",
    "To create a random array we can run the following NumPy-like command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds.random_array(shape=(500, 500), block_size=(100, 100))\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `x` is a 500x500 ds-array of random numbers stored in blocks of 100x100 elements. Note that `x` is not stored in memory. Instead, `random_array` generates the contents of the array in tasks that are usually executed remotely. This allows the creation of really big arrays.\n",
    "\n",
    "The content of `x` is a list of `Futures` that represent the actual data (wherever it is stored).\n",
    "\n",
    "To see this, we can access the `_blocks` field of `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x._blocks[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`block_size` is useful to control the granularity of dislib algorithms.\n",
    "\n",
    "To retrieve the actual contents of `x`, we use `collect`, which synchronizes the data and returns the equivalent NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of creating ds-arrays is using array-like structures like NumPy arrays or lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ds.array([[1, 2, 3], [4, 5, 6]], block_size=(1, 3))\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed arrays can also store sparse data in CSR format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "sp = csr_matrix([[0, 0, 1], [1, 0, 1]])\n",
    "x_sp = ds.array(sp, block_size=(1, 3))\n",
    "x_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `collect` returns a CSR matrix as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sp.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "A typical way of creating ds-arrays is to load data from disk. Dislib currently supports reading data in CSV and SVMLight formats like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds.load_svmlight_file(\"./files/libsvm/1\", block_size=(20, 100), n_features=780, store_sparse=True)\n",
    "\n",
    "print(x)\n",
    "\n",
    "csv = ds.load_txt_file(\"./files/csv/1\", block_size=(500, 122))\n",
    "\n",
    "print(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "Similar to NumPy, ds-arrays support the following types of slicing:\n",
    "\n",
    "(Note that slicing a ds-array creates a new ds-array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds.random_array((50, 50), (10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a single element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a set of rows or a set of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consecutive rows\n",
    "print(x[10:20])\n",
    "\n",
    "# Consecutive columns\n",
    "print(x[:, 10:20])\n",
    "\n",
    "# Non consecutive rows\n",
    "print(x[[3, 7, 22]])\n",
    "\n",
    "# Non consecutive columns\n",
    "print(x[:, [5, 9, 48]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get any set of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0:5, 40:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functions\n",
    "\n",
    "Apart from this, ds-arrays also provide other useful operations like `transpose` and `mean`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.mean(axis=0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.transpose().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning with dislib\n",
    "\n",
    "Dislib provides an estimator-based API very similar to [scikit-learn](https://scikit-learn.org/stable/). To run an algorithm, we first create an estimator. For example, a K-means estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a ds-array with some blob data, and fit the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# create ds-array\n",
    "x, y = make_blobs(n_samples=1500)\n",
    "x_ds = ds.array(x, block_size=(500, 2))\n",
    "\n",
    "km.fit(x_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can make predictions on new (or the same) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = km.predict(x_ds)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_pred` is a ds-array of predicted labels for `x_ds`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "centers = km.centers\n",
    "\n",
    "# set the color of each sample to the predicted label\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y_pred.collect())\n",
    "\n",
    "# plot the computed centers in red\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to call `y_pred.collect()` to retrieve the actual labels and plot them. The rest is the same as if we were using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a more complex example that uses some preprocessing tools.\n",
    "\n",
    "First, we load a classification data set from scikit-learn into ds-arrays. \n",
    "\n",
    "Note that this step is only necessary for demonstration purposes. Ideally, your data should be already loaded in ds-arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "x_train = ds.array(x_train, block_size=(100, 10))\n",
    "y_train = ds.array(y_train.reshape(-1, 1), block_size=(100, 1))\n",
    "\n",
    "x_test = ds.array(x_test, block_size=(100, 10))\n",
    "y_test = ds.array(y_test.reshape(-1, 1), block_size=(100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can see how support vector machines perform in classifying the data. We first fit the model (ignore any warnings in this step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.classification import CascadeSVM\n",
    "\n",
    "csvm = CascadeSVM()\n",
    "\n",
    "csvm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we can make predictions on new data using `csvm.predict()`, or we can get the model accuracy on the test set with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = csvm.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`score` represents the classifier accuracy, however, it is returned as a `Future`. We need to synchronize to get the actual value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycompss.api.api import compss_wait_on\n",
    "\n",
    "print(compss_wait_on(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy should be around 0.6, which is not very good. We can scale the data before classification to improve accuracy. This can be achieved using dislib's `StandardScaler`.\n",
    "\n",
    "The `StandardScaler` provides the same API as other estimators. In this case, however, instead of making predictions on new data, we transform it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# fit the scaler with train data and transform it\n",
    "scaled_train = sc.fit_transform(x_train)\n",
    "\n",
    "# transform test data\n",
    "scaled_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `scaled_train` and `scaled_test` are the scaled samples. Let's see how SVM perfroms now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvm.fit(scaled_train, y_train)\n",
    "score = csvm.score(scaled_test, y_test)\n",
    "print(compss_wait_on(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new accuracy should be around 0.9, which is a great improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session\n",
    "\n",
    "To finish the session, we need to stop PyCOMPSs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipycompss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
